{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar los datos\n",
    "df = pd.read_csv(\"../dataset/train.csv\")\n",
    "\n",
    "# Variables predictoras (mismas usadas en modelos previos)\n",
    "predictors = ['OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'YearBuilt', 'FullBath']\n",
    "X = df[predictors]\n",
    "y = df['SalePrice']\n",
    "\n",
    "# Divisi贸n en conjunto de entrenamiento y prueba (mismo random_state para comparabilidad)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inciso 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy promedio con Validaci贸n Cruzada: 0.77\n"
     ]
    }
   ],
   "source": [
    "# Ejercicio 8: Validaci贸n cruzada\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Convertir SalePrice en categor铆as\n",
    "y_class = pd.qcut(y, q=3, labels=[\"barato\", \"medio\", \"caro\"], duplicates=\"drop\")\n",
    "\n",
    "# Validaci贸n cruzada con Naive Bayes\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "nb = GaussianNB()\n",
    "\n",
    "cv_scores = cross_val_score(nb, X, y_class, cv=kf, scoring='accuracy')  # Evaluamos con accuracy\n",
    "\n",
    "print(f\"Accuracy promedio con Validaci贸n Cruzada: {cv_scores.mean():.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para evaluar la capacidad de generalizaci贸n del modelo de Naive Bayes, se realiz贸 una validaci贸n cruzada con KFold(n_splits=5). Esto permiti贸 medir el rendimiento del modelo en diferentes subconjuntos de los datos, reduciendo el sesgo y asegurando una evaluaci贸n m谩s robusta.\n",
    "\n",
    "El resultado obtenido fue un accuracy promedio de 0.76, lo que significa que el modelo predijo correctamente la categor铆a del precio de las casas en el 76% de los casos.\n",
    "\n",
    "Al compararlo con el modelo sin validaci贸n cruzada, que obtuvo un accuracy de 0.75, se observa una ligera mejora del 1%. Esta diferencia sugiere que el modelo tiene una buena capacidad de generalizaci贸n, ya que su desempe帽o se mantiene estable en diferentes particiones de los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inciso 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Mejor hiperpar谩metro para Naive Bayes: {'var_smoothing': 1e-06}\n",
      " Mejor Accuracy: 0.79\n",
      "\n",
      " Mejor hiperpar谩metro para DecisionTreeRegressor: {'max_depth': 10, 'min_samples_split': 10}\n",
      " Mejor RMSE: 38550.08\n",
      "\n",
      " Mejor hiperpar谩metro para RandomForestRegressor: {'max_depth': 15, 'n_estimators': 50}\n",
      " Mejor RMSE: 32698.72\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "#  1. Ajuste de hiperpar谩metros para Naive Bayes (Clasificaci贸n)\n",
    "param_grid_nb = {'var_smoothing': [1e-9, 1e-8, 1e-7, 1e-6, 1e-5]}\n",
    "nb = GaussianNB()\n",
    "grid_nb = GridSearchCV(nb, param_grid_nb, cv=5, scoring='accuracy')\n",
    "grid_nb.fit(X, y_class)\n",
    "\n",
    "print(f\" Mejor hiperpar谩metro para Naive Bayes: {grid_nb.best_params_}\")\n",
    "print(f\" Mejor Accuracy: {grid_nb.best_score_:.2f}\")\n",
    "\n",
    "#  2. Ajuste de hiperpar谩metros para rbol de Decisi贸n (Regresi贸n)\n",
    "param_grid_dt = {'max_depth': [5, 10, 15, None], 'min_samples_split': [2, 5, 10]}\n",
    "dt = DecisionTreeRegressor(random_state=42)\n",
    "grid_dt = GridSearchCV(dt, param_grid_dt, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_dt.fit(X, y)\n",
    "\n",
    "print(f\"\\n Mejor hiperpar谩metro para DecisionTreeRegressor: {grid_dt.best_params_}\")\n",
    "print(f\" Mejor RMSE: {(-grid_dt.best_score_)**0.5:.2f}\")\n",
    "\n",
    "#  3. Ajuste de hiperpar谩metros para Random Forest (Regresi贸n)\n",
    "param_grid_rf = {'n_estimators': [50, 100, 200], 'max_depth': [5, 10, 15, None]}\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "grid_rf = GridSearchCV(rf, param_grid_rf, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_rf.fit(X, y)\n",
    "\n",
    "print(f\"\\n Mejor hiperpar谩metro para RandomForestRegressor: {grid_rf.best_params_}\")\n",
    "print(f\" Mejor RMSE: {(-grid_rf.best_score_)**0.5:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se realiz贸 ajuste de hiperpar谩metros con validaci贸n cruzada para mejorar el rendimiento de los modelos:\n",
    "\n",
    "Naive Bayes (var_smoothing=1e-9) obtuvo un accuracy de 0.75, sin cambios significativos respecto al modelo original.\n",
    "rbol de Decisi贸n (max_depth=15, min_samples_split=10) mejor贸 su RMSE a 39,741.71, mostrando una mejor capacidad predictiva.\n",
    "Random Forest (max_depth=15, n_estimators=100) obtuvo el mejor RMSE: 29,742.12, superando al rbol de Decisi贸n en precisi贸n.\n",
    "\n",
    "El ajuste de hiperpar谩metros mejor贸 notablemente los modelos de regresi贸n, con Random Forest como el mejor predictor de precios, mientras que Naive Bayes  present贸 mejoras cortas en clasificaci贸n."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inciso 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Modelo: Naive Bayes\n",
      "   - Accuracy: 0.79\n",
      "   - Tiempo de entrenamiento: 0.0044 segundos\n",
      "\n",
      " Modelo: Decision Tree\n",
      "   - Accuracy: 0.76\n",
      "   - Tiempo de entrenamiento: 0.0073 segundos\n",
      "\n",
      " Modelo: Random Forest\n",
      "   - Accuracy: 0.83\n",
      "   - Tiempo de entrenamiento: 0.2139 segundos\n",
      "\n",
      " Comparaci贸n de Modelos:\n",
      "   - Naive Bayes: Accuracy=0.79, Tiempo=0.0044 s\n",
      "   - Decision Tree: Accuracy=0.76, Tiempo=0.0073 s\n",
      "   - Random Forest: Accuracy=0.83, Tiempo=0.2139 s\n",
      "\n",
      "Modelo m谩s preciso: Random Forest\n",
      "Modelo m谩s r谩pido: Naive Bayes\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#  Modelo de rbol de Decisi贸n (Clasificaci贸n)\n",
    "dt_clf = DecisionTreeClassifier(max_depth=10, min_samples_split=5, random_state=42)\n",
    "\n",
    "#  Modelo de Random Forest (Clasificaci贸n)\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "\n",
    "#  Modelo de Naive Bayes (Clasificaci贸n)\n",
    "nb_clf = GaussianNB(var_smoothing=1e-9)\n",
    "\n",
    "#  Funci贸n para entrenar y medir tiempo\n",
    "def train_and_evaluate(model, X_train, X_test, y_train, y_test, name):\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    train_time = time.time() - start_time\n",
    "    \n",
    "    # Predicciones\n",
    "    y_pred = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    print(f\" Modelo: {name}\")\n",
    "    print(f\"   - Accuracy: {acc:.2f}\")\n",
    "    print(f\"   - Tiempo de entrenamiento: {train_time:.4f} segundos\\n\")\n",
    "    return acc, train_time\n",
    "\n",
    "#  Dividir los datos en entrenamiento y prueba (80% - 20%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_class, test_size=0.2, random_state=42)\n",
    "\n",
    "#  Evaluaci贸n de cada modelo\n",
    "acc_nb, time_nb = train_and_evaluate(nb_clf, X_train, X_test, y_train, y_test, \"Naive Bayes\")\n",
    "acc_dt, time_dt = train_and_evaluate(dt_clf, X_train, X_test, y_train, y_test, \"Decision Tree\")\n",
    "acc_rf, time_rf = train_and_evaluate(rf_clf, X_train, X_test, y_train, y_test, \"Random Forest\")\n",
    "\n",
    "#  Comparaci贸n de resultados\n",
    "print(\" Comparaci贸n de Modelos:\")\n",
    "print(f\"   - Naive Bayes: Accuracy={acc_nb:.2f}, Tiempo={time_nb:.4f} s\")\n",
    "print(f\"   - Decision Tree: Accuracy={acc_dt:.2f}, Tiempo={time_dt:.4f} s\")\n",
    "print(f\"   - Random Forest: Accuracy={acc_rf:.2f}, Tiempo={time_rf:.4f} s\")\n",
    "\n",
    "#  An谩lisis de cu谩l es mejor\n",
    "best_model = max(zip([\"Naive Bayes\", \"Decision Tree\", \"Random Forest\"], [acc_nb, acc_dt, acc_rf]), key=lambda x: x[1])\n",
    "fastest_model = min(zip([\"Naive Bayes\", \"Decision Tree\", \"Random Forest\"], [time_nb, time_dt, time_rf]), key=lambda x: x[1])\n",
    "\n",
    "print(\"\\nModelo m谩s preciso:\", best_model[0])\n",
    "print(\"Modelo m谩s r谩pido:\", fastest_model[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se compararon los modelos de Naive Bayes, rbol de Decisi贸n y Random Forest en t茅rminos de precisi贸n (accuracy) y tiempo de entrenamiento:\n",
    "\n",
    "Random Forest fue el mejor modelo para predecir, con un accuracy de 0.85, pero tambi茅n el m谩s lento (0.3607 segundos).\n",
    "Naive Bayes tuvo una buena precisi贸n (0.79) y fue el m谩s r谩pido en entrenar (0.0118 segundos).\n",
    "rbol de Decisi贸n qued贸 en el tercer lugar en precisi贸n (0.75) y tuvo un tiempo intermedio (0.0421 segundos).\n",
    "\n",
    "\n",
    "Random Forest es el modelo m谩s preciso, pero Naive Bayes es el m谩s eficiente en tiempo de entrenamiento, lo que lo hace ideal si se necesita velocidad sin sacrificar demasiada precisi贸n."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
